# Image-Generator

A project that harnesses diffusion models for image generation from user-provided prompts. Leveraging the power of DALL-E 2, Imagen, and Stable Diffusion, I aimed to transform textual descriptions into vivid visual representations. The process involved iteratively refining noise images, gradually adding detail until they converged into realistic scenes. By combining language understanding with diffusion-based image synthesis, I unlocked the ability to create photorealistic artwork from mere words. The challenge lay in deciphering the underlying semantics of prompts and ensuring coherence and fidelity in the generated images.

Technologies used: HTML, CSS, JavaScript, Nodemon, NodeJS, Express, OpenAI
